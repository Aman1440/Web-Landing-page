
<script src="https://cdn.freecodecamp.org/testable-projects-fcc/v1/bundle.js"></script>

<header
        id="header">
    <h2>Let's talk tech!!</h2>  
    <p> We in this page would be focusing on new techs/domains in the CSE/IT field.</p>
    <img 
         id="header-img"
         src="https://st3.depositphotos.com/29881388/37200/v/450/depositphotos_372005414-stock-illustration-idea-technology-sign-logo.jpg">


     
      <div 
           id="main-doc">
        <nav id="nav-bar">
 <h4> Learn!! </h4>
  <ul>
    <li> <a class= "nav-link" href="#dom"> popular domains </a> </li>
    <li> <a class="nav-link" href="#ai">  AI/ML </a> </li>
    <li> <a class="nav-link" href="#app"> App Dev </a> </li>
    <li> <a class="nav-link" href="#web"> Web dev </a> </li>
    <li> <a class="nav-link" href="#tech"> Upcomin Tech </a> </li>
    <li> <a class="nav-link" href="#about"> About Me </a> </li>
    <li> <a class="nav-link" href="#register"> Register </a> </li>
  </ul>
</nav>

</div>   
  </header>

<body>
  <section
        id="dom"   >
    <h4
        id="sub"> Intro!! </h4>
    <p>Technology is now evolving at such a rapid pace that annual predictions of trends can seem out-of-date before they even go live as a published blog post or article. As technology evolves, it enables even faster change and progress, causing an acceleration of the rate of change, until eventually, it will become exponential. 

Technology-based careers don’t change at the same speed, but they do evolve, and the savvy IT professional recognizes that his or her role will not stay the same. And an IT worker of the 21st century will constantly be learning (out of necessity if not desire).

What does this mean for you? It means staying current with technology trends. And it means keeping your eyes on the future, to know which skills you’ll need to know and what types of jobs you want to be qualified to do. Here are eight technology trends you should watch for in 2020, and some of the jobs that will be created by these trends.</p>
    
  </section>
  
  <section 
           id="ai">
    <h4 
        id="sub">Artificial Intelligence</h4>
    <p> Definitions

Computer science defines AI research as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] A more elaborate definition characterizes AI as "a system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation."[62]
Basics

A typical AI analyzes its environment and takes actions that maximize its chance of success.[1] An AI's intended utility function (or goal) can be simple ("1 if the AI wins a game of Go, 0 otherwise") or complex ("Perform actions mathematically similar to ones that succeeded in the past"). Goals can be explicitly defined or induced. If the AI is programmed for "reinforcement learning", goals can be implicitly induced by rewarding some types of behavior or punishing others.[a] Alternatively, an evolutionary system can induce goals by using a "fitness function" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[63] Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[64] Such systems can still be benchmarked if the non-goal system is framed as a system whose "goal" is to successfully accomplish its narrow classification task.[65]

AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions that a mechanical computer can execute.[b] A complex algorithm is often built on top of other, simpler, algorithms. A simple example of an algorithm is the following (optimal for first player) recipe for play at tic-tac-toe:[66]

    If someone has a "threat" (that is, two in a row), take the remaining square. Otherwise,
    if a move "forks" to create two threats at once, play that move. Otherwise,
    take the center square if it is free. Otherwise,
    if your opponent has played in a corner, take the opposite corner. Otherwise,
    take an empty corner if one exists. Otherwise,
    take any empty square.

Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics (strategies, or "rules of thumb", that have worked well in the past), or can themselves write other algorithms. Some of the "learners" described below, including Bayesian networks, decision trees, and nearest-neighbor, could theoretically, (given infinite data, time, and memory) learn to approximate any function, including which combination of mathematical functions would best describe the world[citation needed]. These learners could therefore derive all possible knowledge, by considering every possible hypothesis and matching them against the data. In practice, it is seldom possible to consider every possibility, because of the phenomenon of "combinatorial explosion", where the time needed to solve a problem grows exponentially. Much of AI research involves figuring out how to identify and avoid considering a broad range of possibilities unlikely to be beneficial.[67][68] For example, when viewing a map and looking for the shortest driving route from Denver to New York in the East, one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus, an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered. </p>
    </section>
  
  <section 
           id="app">
    <h4 
        id="sub">App development</h4>
    <p>Mobile app development is the act or process by which a mobile app is developed for mobile devices, such as personal digital assistants, enterprise digital assistants or mobile phones. These applications can be pre-installed on phones during manufacturing platforms, or delivered as web applications using server-side or client-side processing (e.g., JavaScript) to provide an "application-like" experience within a Web browser. Application software developers also must consider a long array of screen sizes, hardware specifications, and configurations because of intense competition in mobile software and changes within each of the platforms. Mobile app development has been steadily growing, in revenues and jobs created. A 2013 analyst report estimates there are 529,000 direct app economy jobs within the EU then 28 members (including the UK), 60 percent of which are mobile app developers.[1]

As part of the development process, mobile user interface (UI) design is also essential in the creation of mobile apps. Mobile UI considers constraints, contexts, screen, input, and mobility as outlines for design. The user is often the focus of interaction with their device, and the interface entails components of both hardware and software. User input allows for the users to manipulate a system, and device's output allows the system to indicate the effects of the users' manipulation. Mobile UI design constraints include limited attention and form factors, such as a mobile device's screen size for a user's hand(s). Mobile UI contexts signal cues from user activity, such as location and scheduling that can be shown from user interactions within a mobile app. Overall, mobile UI design's goal is mainly for an understandable, user-friendly interface. The UI of mobile apps should: consider users' limited attention, minimize keystrokes, and be task-oriented with a minimum set of functions. This functionality is supported by mobile enterprise application platforms or integrated development environments (IDEs).

Mobile UIs, or front-ends, rely on mobile back-ends to support access to enterprise systems. The mobile back-end facilitates data routing, security, authentication, authorization, working off-line, and service orchestration. This functionality is supported by a mix of middleware components including mobile app server, mobile backend as a service (MBaaS), and service-oriented architecture (SOA) infrastructure.

Mobile app development is becoming more critical for many businesses with more than 3 billion people worldwide using smartphones, more than 1.5 billion using tablets as of 2019. Users, on average, spend 90 percent of their mobile time in apps and there are more than 700 million apps downloads from various app stores.[2] </p>
    </section>
  
  <section 
           id="web">
    <h4 
        id="sub"> Web Development </h4>
    <p>  Web development is the work involved in developing a website for the Internet (World Wide Web) or an intranet (a private network).[1] Web development can range from developing a simple single static page of plain text to complex web-based internet applications (web apps), electronic businesses, and social network services. A more comprehensive list of tasks to which web development commonly refers, may include web engineering, web design, web content development, client liaison, client-side/server-side scripting, web server and network security configuration, and e-commerce development.

Among web professionals, "web development" usually refers to the main non-design aspects of building websites: writing markup and coding.[2] Web development may use content management systems (CMS) to make content changes easier and available with basic technical skills.

For larger organizations and businesses, web development teams can consist of hundreds of people (web developers) and follow standard methods like Agile methodologies while developing websites. Smaller organizations may only require a single permanent or contracting developer, or secondary assignment to related job positions such as a graphic designer or information systems technician. Web development may be a collaborative effort between departments rather than the domain of a designated department. There are three kinds of web developer specialization: front-end developer, back-end developer, and full-stack developer. Front-end developers are responsible for behavior and visuals that run in the user browser, while back-end developers deal with the servers. 
    </p>
    </section>
  
  <section 
           id="tech">
    <h4
        id="sub"> Robotic Process Automation or RPA </h4>
          <p>
            

Like AI and Machine Learning, Robotic Process Automation, or RPA, is another technology that is automating jobs. RPA is the use of software to automate business processes such as interpreting applications, processing transactions, dealing with data, and even replying to emails. RPA automates repetitive tasks that people used to do. These are not just the menial tasks of a low-paid worker: up to 45 percent of the activities we do can be automated, including the work of financial managers, doctors, and CEOs.

Although Forrester Research estimates RPA automation will threaten the livelihood of 230 million or more knowledge workers or approximately 9 percent of the global workforce, RPA is also creating new jobs while altering existing jobs. McKinsey finds that less than 5 percent of occupations can be totally automated, but about 60 percent can be partially automated.

For you as an IT professional looking to the future and trying to understand technology trends, RPA offers plenty of career opportunities, including developer, project manager, business analyst, solution architect, and consultant. And these jobs pay well. SimplyHired.com says the average RPA salary is $73,861, but that is the average compiled from salaries for junior-level developers up to senior solution architects, with the top 10 percent earning over $141,000 annually. So, if you’re keen on learning and pursuing a career in RPA, the Introduction to Robotic Process Automation (RPA) course should be the next step you take to kickstart an RPA career. </p>
<h4
    id="sub"> Edge Computing</h4>
    <p>

Formerly a technology trend to watch, cloud computing has become mainstream, with major players AWS (Amazon Web Services), Microsoft Azure and Google Cloud dominating the market. The adoption of cloud computing is still growing, as more and more businesses migrate to a cloud solution. But it’s no longer the emerging technology. 

As the quantity of data we’re dealing with continues to increase, we’ve realized the shortcomings of cloud computing in some situations. Edge computing is designed to help solve some of those problems as a way to bypass the latency caused by cloud computing and getting data to a data center for processing. It can exist “on the edge,” if you will, closer to where computing needs to happen. For this reason, edge computing can be used to process time-sensitive data in remote locations with limited or no connectivity to a centralized location. In those situations, edge computing can act like mini datacenters. Edge computing will increase as the use of the Internet of Things (IoT) devices increases. By 2022, the global edge computing market is expected to reach $6.72 billion. As with any growing market, this will create various jobs, primarily for software engineers. </p>
        
    </section>
  
  <section 
           id="about">
    <h4> The Community I am a part off! </h4>
    <iframe
            id="video"
            width="560" height="315" src="https://www.youtube.com/embed/OhwTI0qfX7M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </section>
  
</body>

<footer> 
  <br>
  <form id="form">
  <label for="email" id="register"> Email: </label>
    <input type="email" name="email" id="email" placeholder="email">
  <label for="submit"></label>
    <input type="submit" name="submit" id="submit" value="Submit" placeholder="submit">
    </form>
</footer>
